{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterative Solvers Stability (Advanced)\n",
    "\n",
    "## Objective\n",
    "\n",
    "Study stability of iterative methods:\n",
    "- Jacobi and Gauss-Seidel\n",
    "- Conjugate Gradient\n",
    "- Convergence and conditioning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(42)\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Jacobi Iteration\n",
    "\n",
    "For Ax = b, iterate: $x^{(k+1)} = D^{-1}(b - (L+U)x^{(k)})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jacobi(A, b, max_iter=100, tol=1e-10):\n",
    "    \"\"\"Jacobi iteration.\"\"\"\n",
    "    n = len(b)\n",
    "    x = np.zeros(n)\n",
    "    D = np.diag(np.diag(A))\n",
    "    R = A - D\n",
    "    \n",
    "    errors = []\n",
    "    for k in range(max_iter):\n",
    "        x_new = np.linalg.solve(D, b - R @ x)\n",
    "        errors.append(np.linalg.norm(A @ x_new - b))\n",
    "        if errors[-1] < tol:\n",
    "            break\n",
    "        x = x_new\n",
    "    \n",
    "    return x, errors\n",
    "\n",
    "# Test on diagonally dominant matrix\n",
    "n = 50\n",
    "A = np.random.randn(n, n)\n",
    "A = A + 10 * np.eye(n)  # Make diagonally dominant\n",
    "b = np.random.randn(n)\n",
    "\n",
    "x_jacobi, errors_jacobi = jacobi(A, b)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.semilogy(errors_jacobi, linewidth=2)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Residual norm\")\n",
    "plt.title(\"Jacobi Convergence\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.savefig(\"../plots/09_jacobi_convergence.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"Converged in {len(errors_jacobi)} iterations\")\n",
    "print(f\"Final residual: {errors_jacobi[-1]:.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conjugate Gradient\n",
    "\n",
    "For symmetric positive definite systems, CG is optimal Krylov method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conjugate_gradient(A, b, max_iter=None, tol=1e-10):\n",
    "    \"\"\"Conjugate gradient method.\"\"\"\n",
    "    n = len(b)\n",
    "    if max_iter is None:\n",
    "        max_iter = n\n",
    "    \n",
    "    x = np.zeros(n)\n",
    "    r = b - A @ x\n",
    "    p = r.copy()\n",
    "    \n",
    "    errors = []\n",
    "    \n",
    "    for k in range(max_iter):\n",
    "        Ap = A @ p\n",
    "        alpha = np.dot(r, r) / np.dot(p, Ap)\n",
    "        x = x + alpha * p\n",
    "        r_new = r - alpha * Ap\n",
    "        \n",
    "        errors.append(np.linalg.norm(r_new))\n",
    "        \n",
    "        if errors[-1] < tol:\n",
    "            break\n",
    "        \n",
    "        beta = np.dot(r_new, r_new) / np.dot(r, r)\n",
    "        p = r_new + beta * p\n",
    "        r = r_new\n",
    "    \n",
    "    return x, errors\n",
    "\n",
    "# Test on SPD matrix\n",
    "n = 50\n",
    "A = np.random.randn(n, n)\n",
    "A = A.T @ A + np.eye(n)  # Make SPD\n",
    "b = np.random.randn(n)\n",
    "\n",
    "x_cg, errors_cg = conjugate_gradient(A, b)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.semilogy(errors_cg, linewidth=2)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Residual norm\")\n",
    "plt.title(\"Conjugate Gradient Convergence\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.savefig(\"../plots/09_cg_convergence.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"CG converged in {len(errors_cg)} iterations\")\n",
    "print(f\"Theoretical max: {n} iterations\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **Iterative methods** useful for large sparse systems\n",
    "2. **Convergence** depends on matrix properties (diagonal dominance, SPD)\n",
    "3. **CG is optimal** for SPD systems\n",
    "4. **Preconditioning** can dramatically improve convergence"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}