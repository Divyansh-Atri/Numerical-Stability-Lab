{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mixed-Precision Experiments (Advanced)\n",
    "\n",
    "## Objective\n",
    "\n",
    "Study error accumulation in different precisions:\n",
    "- float32 vs float64 error growth\n",
    "- Mixed-precision iterative refinement\n",
    "- Precision-dependent stability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "from utils.floating_point_tools import naive_sum, kahan_sum\n",
    "from utils.linear_algebra_utils import gaussian_elimination_with_pivoting\n",
    "\n",
    "np.random.seed(42)\n",
    "plt.rcParams[\"figure.figsize\"] = (12, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision Comparison\n",
    "\n",
    "| Type | Bits | Mantissa | Exponent | \u03b5_mach |\n",
    "|------|------|----------|----------|--------|\n",
    "| float32 | 32 | 23 | 8 | 2^-23 \u2248 1.2e-7 |\n",
    "| float64 | 64 | 52 | 11 | 2^-52 \u2248 2.2e-16 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare summation in different precisions\n",
    "sizes = np.logspace(2, 6, 15).astype(int)\n",
    "errors_32 = []\n",
    "errors_64 = []\n",
    "\n",
    "for n in sizes:\n",
    "    arr = np.random.randn(n)\n",
    "    exact = np.sum(arr.astype(np.float128))\n",
    "    \n",
    "    sum_32 = np.sum(arr.astype(np.float32))\n",
    "    sum_64 = np.sum(arr.astype(np.float64))\n",
    "    \n",
    "    errors_32.append(abs(sum_32 - exact) / abs(exact))\n",
    "    errors_64.append(abs(sum_64 - exact) / abs(exact))\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.loglog(sizes, errors_32, \"o-\", label=\"float32\", linewidth=2)\n",
    "plt.loglog(sizes, errors_64, \"s-\", label=\"float64\", linewidth=2)\n",
    "plt.loglog(sizes, sizes * np.finfo(np.float32).eps, \"--\", label=r\"$n \\varepsilon_{32}$\", alpha=0.5)\n",
    "plt.loglog(sizes, sizes * np.finfo(np.float64).eps, \"--\", label=r\"$n \\varepsilon_{64}$\", alpha=0.5)\n",
    "plt.xlabel(\"Array size\")\n",
    "plt.ylabel(\"Relative error\")\n",
    "plt.title(\"Summation Error: float32 vs float64\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.savefig(\"../plots/07_mixed_precision_summation.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Iterative Refinement\n",
    "\n",
    "Solve Ax = b using mixed precision:\n",
    "1. Compute approximate solution in low precision\n",
    "2. Compute residual in high precision\n",
    "3. Solve for correction\n",
    "4. Refine solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mixed-precision iterative refinement\n",
    "def iterative_refinement(A, b, max_iter=5):\n",
    "    \"\"\"Solve Ax=b with iterative refinement.\"\"\"\n",
    "    n = len(b)\n",
    "    \n",
    "    # Initial solve in float32\n",
    "    A_32 = A.astype(np.float32)\n",
    "    b_32 = b.astype(np.float32)\n",
    "    x = np.linalg.solve(A_32, b_32).astype(np.float64)\n",
    "    \n",
    "    errors = []\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        # Compute residual in float64\n",
    "        r = b - A @ x\n",
    "        \n",
    "        # Solve for correction in float32\n",
    "        delta = np.linalg.solve(A_32, r.astype(np.float32)).astype(np.float64)\n",
    "        \n",
    "        # Update solution\n",
    "        x = x + delta\n",
    "        \n",
    "        # Track error\n",
    "        errors.append(np.linalg.norm(r))\n",
    "    \n",
    "    return x, errors\n",
    "\n",
    "# Test\n",
    "n = 50\n",
    "A = np.random.randn(n, n)\n",
    "x_true = np.random.randn(n)\n",
    "b = A @ x_true\n",
    "\n",
    "x_refined, residuals = iterative_refinement(A, b)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.semilogy(residuals, \"o-\", linewidth=2)\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Residual norm\")\n",
    "plt.title(\"Iterative Refinement Convergence\")\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.savefig(\"../plots/07_iterative_refinement.png\", dpi=150, bbox_inches=\"tight\")\n",
    "plt.show()\n",
    "\n",
    "print(f\"Final error: {np.linalg.norm(x_refined - x_true) / np.linalg.norm(x_true):.2e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Takeaways\n",
    "\n",
    "1. **float32 has ~7 decimal digits**, float64 has ~16\n",
    "2. **Error scales with precision**: \u03b5_32 \u2248 10^9 \u00d7 \u03b5_64\n",
    "3. **Mixed precision** can improve efficiency while maintaining accuracy\n",
    "4. **Iterative refinement** recovers full precision from low-precision solve"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}